services:
  # --- Ollama inference server ---
  ollama:
    image: ollama/ollama:latest
    container_name: ollama
    ports:
      - "11434:11434"          # expose to localhost
    volumes:
      - ollama:/root/.ollama   # persist models/cache
    networks:
      - spring-ai-net
    healthcheck:
      test: ["CMD", "curl", "-fsS", "http://localhost:11434/api/tags"]
      interval: 5s
      timeout: 3s
      retries: 20
      start_period: 10s
    # default command is `ollama serve`

  # Optional one-shot that pulls the model at startup and then exits
  ollama-pull:
    image: ollama/ollama:latest
    depends_on:
      ollama:
        condition: service_healthy
    networks:
      - spring-ai-net
    volumes:
      - ollama:/root/.ollama
    entrypoint: ["/bin/sh", "-lc"]
    command: >
      "ollama pull llama3.2 && echo 'llama3.2 ready'"

  # --- Your Spring Boot app ---
  spring-app:
    build: .
    container_name: spring-ai-app
    ports:
      - "8081:8081"
    environment:
      # talk to Ollama by container name on the internal network
      SPRING_AI_OLLAMA_BASE_URL: http://ollama:11434

      # (optional) set default chat model to llama3.2 via env
      SPRING_AI_OLLAMA_CHAT_OPTIONS_MODEL: llama3.2

      # (keep your existing settings)
      SPRING_AI_VECTORSTORE_CHROMA_CLIENT_HOST: http://chroma
      SPRING_AI_VECTORSTORE_CHROMA_CLIENT_PORT: "8000"
    networks:
      - spring-ai-net
    depends_on:
      ollama:
        condition: service_healthy
      ollama-pull:
        condition: service_completed_successfully

  # (optional) Chroma example, only if you actually use it
  # chroma:
  #   image: chromadb/chroma:latest
  #   container_name: chroma
  #   ports:
  #     - "8000:8000"
  #   networks:
  #     - spring-ai-net

networks:
  spring-ai-net:
    driver: bridge

volumes:
  ollama:
